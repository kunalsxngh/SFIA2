# DevOps Core Practical Project
As part of my training to become a DevOps engineer at QA, I was asked to create an application that generates "Objects" upon a set of predefined rules. This application must be based on a service-orientated architecture, composed of four services.

These four services all rely on each other to function. Service one handles the front-end of the program while communicating with all the other services to produce an output to the user, while storing them in an SQL database. Service 2 and 3 produce the randomly generated objects that are then used to create the final output for the user in service 4.

The complexity of the program itself is rather simple, since an emphasis was placed instead on ensuring the correct technologies were implemented and functioning properly. These technologies include but are not limited to: 
 - Containerisation of modules via **Docker**, orchestrated by **Docker Swarm**
 - Server provision and task automation via **Ansible**
 - Load balancing via **NGINX**, allowing for the use of an reverse proxy
 - Automated testing and deployment via **Jenkins**, executed by GitHub **Webhooks**

# Contents

To be added at the end

# Software Design

This section will explore the varies methods used to plan the development of this project.

## Planning

Before any development commenced, a Kanban board was created on Trello. The board holds information on what's currently in development, who is working on it and where something is in the process. It also stores project resources, User stories and Minimum viable product (MVP) requirements.

![TrelloBoard](images/TrelloBoard.png)

## Database

As this program only makes use of one table, an entity-relation diagram cannot be created, and so an entity diagram below is shown instead.

![EntityDiagram](images/EntityDiagram.png)

The table has three attributes:
- id : Uniquely represents each entity and is the primary key of the table
- Numberplate : Stores the numberplate of the car
- Color : Stores the color of the car 

## Risk Assessment

A risk assessment was also carried out as a tool for identifying and preventing potential project risks. However, unanticipated risks are expected as in any project, and so risks as added while the program is in development. The two images below show the risks before and after development:

Before development:

![FirstRA](images/FirstRA.png)

After development:

![SecondRA](images/SecondRA.png)


## Test plans

Test plans were also designed to ensure all aspects of the program were tested. The table below explains how each service is expected to be tested.

![TestPlan](images/TestPlan.png)

# Continuous Deployment and Integration

A CI-CD pipeline was implemented in this project to allow for the automation of development in to live builds. The diagram below explains the pipeline very well:

![CI-CD Pipeline](images/CI-CD.png)


To implement this pipeline in to the project, a Jenkins pipeline was set up:

![Jenkins Pipeline](images/JenkinsPipeline.png)

As shown above, there are 5 main stages in the pipeline:
1) Testing: All tests are carried out and reports are produced for Jenkins to view
2) Push image: Docker credentials are uploaded from Jenkins, which are then used to push the images of the services once they are built
3) Configure Swarm and Ansible: The Ansible playbook is run, which starts NGINX, initiailizes Docker Swarm and adds the managers and worker nodes.
4) Deploy application: The stack is deployed with the Docker-Compose file, and the environmental variables are also set
5) Post Actions: The test reports are posted to view the tests that have passed and test coverage.

# Architectures

### Docker-Compose
The first main framework used is Docker-Compose. Docker compose is a tool used to define and run multiple containers at the same time, within the same network by running one command: `docker-compose up`. These containers are all generated by images created by individual dockerfiles which need to be placed in the directory of each service.

### Docker Swarm
As this project is containerised, it benefits from the ability to be moved and scaled across different nodes. This is because containers ensure that the application is run in the same where anywhere. A downside to this scalability is the maintainence costs it brings, however these costs can be avoided by bringing an orchestrator, such as Docker Swarm.

Docker Swarm automates the maintenance this project by replacing failed containers automatically, and managing rollout of updates. Docker Swarm uses all the virtual machines in this project and forms one system that users can access. This can be shown below in the diagram:

![swarm](images/swarm.png)